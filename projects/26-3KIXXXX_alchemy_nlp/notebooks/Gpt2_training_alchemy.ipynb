{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f9f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Prepare Data\n",
    "def load_and_clean_text(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text).strip()\n",
    "        text = re.sub(r'\\d+', '', text)              # Remove digits\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()     # Normalize whitespace\n",
    "        return text.lower()                          # Lowercase for consistency\n",
    "\n",
    "    df['cleaned'] = df['Text'].fillna(\"\").apply(clean_text)\n",
    "    return df['cleaned'].tolist()  # Return as list of lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbda48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV file (upload manually in Colab or mount Google Drive)\n",
    "text = load_and_clean_text(\"alchtexts2_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ljk-NVybhFtK",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to .txt for dataset loading\n",
    "with open(\"alchemy_cleaned.txt\", \"w\") as f:\n",
    "    for line in text:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26675869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Tokenizer and Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Dataset for Fine-Tuning\n",
    "def load_dataset_for_training(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    return dataset, data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df212bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, data_collator = load_dataset_for_training(\"alchemy_cleaned.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BUh9PFTw630i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Training Setup\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-alchemy\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    "    report_to=\"wandb\",  # Enables wandb logging\n",
    "    run_name=\"gpt2-alchemy-run1\"  # Optional: useful to track runs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae38063",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e446943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jNvda0Acw1hF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5: Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Perplexity:\", math.exp(eval_results[\"eval_loss\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-alchemy\")\n",
    "tokenizer.save_pretrained(\"./gpt2-alchemy\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
