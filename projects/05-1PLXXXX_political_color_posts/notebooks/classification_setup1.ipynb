{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tpxrwoR5dh1d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEPOJV8LYNGX"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/IvaroEkel/Probabilistic-Machine-Learning_lecture-PROJECTS/refs/heads/main/projects/05-1PLXXXX_political_color_posts/data/final-features-rgb.csv'\n",
        "df = pd.read_csv(url)\n",
        "X = df[[f'feature_{i}' for i in range(12)]]\n",
        "y = df['party']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oLJjm3uTgZNM"
      },
      "outputs": [],
      "source": [
        "# extract model metrics\n",
        "def extract_metrics(report):\n",
        "    metrics = {}\n",
        "    lines = report.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        if line.startswith(' ') and len(line.split()) > 1:\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 5:\n",
        "                label = parts[0]\n",
        "                try:\n",
        "                    precision = float(parts[1])\n",
        "                    recall = float(parts[2])\n",
        "                    f1_score = float(parts[3])\n",
        "                    support = int(parts[4])\n",
        "                    metrics[label] = {\n",
        "                        'Precision': precision,\n",
        "                        'Recall': recall,\n",
        "                        'F1-Score': f1_score,\n",
        "                        'Support': support\n",
        "                    }\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MDOB3pYG3U",
        "outputId": "5908ccea-5c71-4068-a696-6d3ccdb6fa4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Random Forest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.74      0.45      0.56       411\n",
            "         cdu       0.47      0.61      0.53      1138\n",
            "         csu       0.54      0.69      0.60      1355\n",
            "         fdp       0.88      0.79      0.83       841\n",
            "      gruene       0.86      0.51      0.64       532\n",
            "       linke       0.52      0.48      0.50       845\n",
            "         spd       0.58      0.37      0.46       563\n",
            "\n",
            "    accuracy                           0.59      5685\n",
            "   macro avg       0.65      0.56      0.59      5685\n",
            "weighted avg       0.62      0.59      0.59      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Random Forest\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "metrics_rf = extract_metrics(report_rf)\n",
        "print(\"=== Random Forest ===\")\n",
        "print(report_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P41epFqdYA0-",
        "outputId": "02d5f87b-3b3f-401d-c3e7-66cdbf641fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Support Vector Machine (SVM) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.44      0.54      0.48       411\n",
            "         cdu       0.44      0.63      0.52      1138\n",
            "         csu       0.60      0.49      0.54      1355\n",
            "         fdp       0.85      0.76      0.80       841\n",
            "      gruene       0.64      0.59      0.62       532\n",
            "       linke       0.54      0.27      0.36       845\n",
            "         spd       0.36      0.49      0.41       563\n",
            "\n",
            "    accuracy                           0.54      5685\n",
            "   macro avg       0.55      0.54      0.53      5685\n",
            "weighted avg       0.56      0.54      0.54      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. Support Vector Machine (SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "y_pred_svm = svm.predict(X_test_scaled)\n",
        "report_svm = classification_report(y_test, y_pred_svm)\n",
        "metrics_svm = extract_metrics(report_svm)\n",
        "print(\"=== Support Vector Machine (SVM) ===\")\n",
        "print(report_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9EPf5TYc1t4",
        "outputId": "04c5b626-b0bd-4deb-d5c1-94f72e11084c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MLP Classifier ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.54      0.52      0.53       411\n",
            "         cdu       0.46      0.64      0.53      1138\n",
            "         csu       0.56      0.58      0.57      1355\n",
            "         fdp       0.87      0.79      0.83       841\n",
            "      gruene       0.71      0.56      0.63       532\n",
            "       linke       0.49      0.43      0.46       845\n",
            "         spd       0.49      0.31      0.38       563\n",
            "\n",
            "    accuracy                           0.57      5685\n",
            "   macro avg       0.59      0.55      0.56      5685\n",
            "weighted avg       0.58      0.57      0.57      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. MLP Classifier\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=1000,\n",
        "    alpha=0.0001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "report_mlp = classification_report(y_test, y_pred_mlp)\n",
        "metrics_mlp = extract_metrics(report_mlp)\n",
        "print(\"=== MLP Classifier ===\")\n",
        "print(report_mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL7AP-5ig2Qc",
        "outputId": "531a57f8-2afd-4ac6-e4bc-d8c052a7d18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Modellvergleich ===\n",
            "         Random Forest   SVM   MLP  Average\n",
            "afd               0.56  0.48  0.53     0.52\n",
            "cdu               0.53  0.52  0.53     0.53\n",
            "csu               0.60  0.54  0.57     0.57\n",
            "fdp               0.83  0.80  0.83     0.82\n",
            "gruene            0.64  0.62  0.63     0.63\n",
            "linke             0.50  0.36  0.46     0.44\n",
            "spd               0.46  0.41  0.38     0.42\n",
            "Average           0.59  0.53  0.56     0.56\n"
          ]
        }
      ],
      "source": [
        "metrics_all = {}\n",
        "\n",
        "for label in metrics_rf:\n",
        "    metrics_all[label] = {\n",
        "        'Random Forest': metrics_rf[label]['F1-Score'],\n",
        "        'SVM': metrics_svm[label]['F1-Score'],\n",
        "        'MLP': metrics_mlp[label]['F1-Score']\n",
        "    }\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_all).T\n",
        "\n",
        "metrics_df[\"Average\"] = metrics_df.mean(axis=1)\n",
        "\n",
        "average_row = metrics_df.mean(numeric_only=True)\n",
        "average_row.name = \"Average\"\n",
        "metrics_df = pd.concat([metrics_df, average_row.to_frame().T])\n",
        "\n",
        "#Output\n",
        "print(\"=== Modellvergleich ===\")\n",
        "print(metrics_df.round(2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}