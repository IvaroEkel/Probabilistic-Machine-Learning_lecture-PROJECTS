{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1746097796590,
     "user": {
      "displayName": "Lukas Pasold",
      "userId": "06420352777570022367"
     },
     "user_tz": -120
    },
    "id": "tpxrwoR5dh1d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1746097798168,
     "user": {
      "displayName": "Lukas Pasold",
      "userId": "06420352777570022367"
     },
     "user_tz": -120
    },
    "id": "oEPOJV8LYNGX"
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv('final-features-rgb.csv')\n",
    "# load the file from the directory ../data/, by using the os module to specify the path of the current file\n",
    "df = pd.read_csv(os.path.join(os.path.dirname('__file__'), '../data/final-features-rgb.csv'))\n",
    "\n",
    "X = df[[f'feature_{i}' for i in range(12)]]\n",
    "y = df['party']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
=======
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxrwoR5dh1d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBClassifier"
      ]
    },
>>>>>>> 15865582465bc208a31ae6df46c4c4be5ca9ebe6
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEPOJV8LYNGX"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/IvaroEkel/Probabilistic-Machine-Learning_lecture-PROJECTS/refs/heads/main/projects/05-1PLXXXX_political_color_posts/data/final-features-rgb.csv'\n",
        "df = pd.read_csv(url)\n",
        "X = df[[f'feature_{i}' for i in range(12)]]\n",
        "y = df['party']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLJjm3uTgZNM"
      },
      "outputs": [],
      "source": [
        "# extract model metrics\n",
        "def extract_metrics(report):\n",
        "    metrics = {}\n",
        "    lines = report.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        if line.startswith(' ') and len(line.split()) > 1:\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 5:\n",
        "                label = parts[0]\n",
        "                try:\n",
        "                    precision = float(parts[1])\n",
        "                    recall = float(parts[2])\n",
        "                    f1_score = float(parts[3])\n",
        "                    support = int(parts[4])\n",
        "                    metrics[label] = {\n",
        "                        'Precision': precision,\n",
        "                        'Recall': recall,\n",
        "                        'F1-Score': f1_score,\n",
        "                        'Support': support\n",
        "                    }\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MDOB3pYG3U",
        "outputId": "92f51963-a75e-4d3d-d5a1-f0c8db2a293c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Random Forest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.74      0.45      0.56       411\n",
            "         cdu       0.47      0.61      0.53      1138\n",
            "         csu       0.54      0.69      0.60      1355\n",
            "         fdp       0.88      0.79      0.83       841\n",
            "      gruene       0.86      0.51      0.64       532\n",
            "       linke       0.52      0.48      0.50       845\n",
            "         spd       0.58      0.37      0.46       563\n",
            "\n",
            "    accuracy                           0.59      5685\n",
            "   macro avg       0.65      0.56      0.59      5685\n",
            "weighted avg       0.62      0.59      0.59      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Random Forest\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "metrics_rf = extract_metrics(report_rf)\n",
        "print(\"=== Random Forest ===\")\n",
        "print(report_rf)"
      ]
<<<<<<< HEAD
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746097799752,
     "user": {
      "displayName": "Lukas Pasold",
      "userId": "06420352777570022367"
     },
     "user_tz": -120
    },
    "id": "oLJjm3uTgZNM"
   },
   "outputs": [],
   "source": [
    "# extract model metrics\n",
    "def extract_metrics(report):\n",
    "    metrics = {}\n",
    "    lines = report.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.startswith(' ') and len(line.split()) > 1:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 5:\n",
    "                label = parts[0]\n",
    "                try:\n",
    "                    precision = float(parts[1])\n",
    "                    recall = float(parts[2])\n",
    "                    f1_score = float(parts[3])\n",
    "                    support = int(parts[4])\n",
    "                    metrics[label] = {\n",
    "                        'Precision': precision, # another name for precision is positive predictive value. It is related to the type-I error rate.\n",
    "                        'Recall': recall, # another name for recall is sensitivity. It corresponds to the complement of the type-II error rate\n",
    "                        'F1-Score': f1_score, # \n",
    "                        'Support': support # \n",
    "                    }\n",
    "                except ValueError:\n",
    "                    continue  \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10130,
     "status": "ok",
     "timestamp": 1746097811728,
     "user": {
      "displayName": "Lukas Pasold",
      "userId": "06420352777570022367"
     },
     "user_tz": -120
    },
    "id": "d2MDOB3pYG3U",
    "outputId": "ea35b411-c73b-4f56-a6b1-ad5b0da31e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afd       0.74      0.45      0.56       411\n",
      "         cdu       0.47      0.61      0.53      1138\n",
      "         csu       0.54      0.69      0.60      1355\n",
      "         fdp       0.88      0.79      0.83       841\n",
      "      gruene       0.86      0.51      0.64       532\n",
      "       linke       0.52      0.48      0.50       845\n",
      "         spd       0.58      0.37      0.46       563\n",
      "\n",
      "    accuracy                           0.59      5685\n",
      "   macro avg       0.65      0.56      0.59      5685\n",
      "weighted avg       0.62      0.59      0.59      5685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Random Forest \n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "metrics_rf = extract_metrics(report_rf)\n",
    "print(\"=== Random Forest ===\")\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
=======
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. XGBoost\n",
        "# Encode the target variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb.fit(X_train, y_train_encoded)\n",
        "y_pred_xgb_encoded = xgb.predict(X_test)\n",
        "\n",
        "# Decode the predictions back to original labels for classification report\n",
        "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
        "\n",
        "report_xgb = classification_report(y_test, y_pred_xgb)\n",
        "metrics_xgb = extract_metrics(report_xgb)\n",
        "print(\"=== XGBoost ===\")\n",
        "print(report_xgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1bHkTw0h71i",
        "outputId": "687bdfd3-9c09-46ea-a68f-3186f44cc501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:33:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== XGBoost ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.76      0.46      0.57       411\n",
            "         cdu       0.49      0.63      0.55      1138\n",
            "         csu       0.57      0.70      0.62      1355\n",
            "         fdp       0.90      0.81      0.85       841\n",
            "      gruene       0.80      0.59      0.68       532\n",
            "       linke       0.55      0.50      0.52       845\n",
            "         spd       0.58      0.41      0.48       563\n",
            "\n",
            "    accuracy                           0.62      5685\n",
            "   macro avg       0.66      0.59      0.61      5685\n",
            "weighted avg       0.64      0.62      0.62      5685\n",
            "\n"
          ]
        }
      ]
>>>>>>> 15865582465bc208a31ae6df46c4c4be5ca9ebe6
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P41epFqdYA0-",
        "outputId": "470eda09-55c0-401b-b0b6-937e07d8c25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Support Vector Machine (SVM) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.44      0.54      0.48       411\n",
            "         cdu       0.44      0.63      0.52      1138\n",
            "         csu       0.60      0.49      0.54      1355\n",
            "         fdp       0.85      0.76      0.80       841\n",
            "      gruene       0.64      0.59      0.62       532\n",
            "       linke       0.54      0.27      0.36       845\n",
            "         spd       0.36      0.49      0.41       563\n",
            "\n",
            "    accuracy                           0.54      5685\n",
            "   macro avg       0.55      0.54      0.53      5685\n",
            "weighted avg       0.56      0.54      0.54      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Support Vector Machine (SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "y_pred_svm = svm.predict(X_test_scaled)\n",
        "report_svm = classification_report(y_test, y_pred_svm)\n",
        "metrics_svm = extract_metrics(report_svm)\n",
        "print(\"=== Support Vector Machine (SVM) ===\")\n",
        "print(report_svm)"
      ]
    },
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MLP Classifier ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         afd       0.54      0.52      0.53       411\n",
      "         cdu       0.46      0.64      0.53      1138\n",
      "         csu       0.56      0.58      0.57      1355\n",
      "         fdp       0.87      0.79      0.83       841\n",
      "      gruene       0.71      0.56      0.63       532\n",
      "       linke       0.49      0.43      0.46       845\n",
      "         spd       0.49      0.31      0.38       563\n",
      "\n",
      "    accuracy                           0.57      5685\n",
      "   macro avg       0.59      0.55      0.56      5685\n",
      "weighted avg       0.58      0.57      0.57      5685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. MLP Classifier \n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    alpha=0.0001,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "report_mlp = classification_report(y_test, y_pred_mlp)\n",
    "metrics_mlp = extract_metrics(report_mlp)\n",
    "print(\"=== MLP Classifier ===\")\n",
    "print(report_mlp)\n",
    "\n",
    "# plot the decision boundaries\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y, palette='Set1', alpha=0.7)\n",
    "    \n",
    "    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# use only the first two features for visualization\n",
    "X_train_2d = X_train.iloc[:, :2]\n",
    "X_test_2d = X_test.iloc[:, :2]\n",
    "# Train the models on the 2D data\n",
    "rf_2d = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_2d.fit(X_train_2d, y_train)\n",
    "svm_2d = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "svm_2d.fit(X_train_2d, y_train)\n",
    "mlp_2d = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=1000,\n",
    "    alpha=0.0001,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_2d.fit(X_train_2d, y_train)\n",
    "# Plot the decision boundaries\n",
    "plot_decision_boundaries(X_train_2d, y_train, rf_2d, \"Random Forest Decision Boundary (2D)\")\n",
    "plot_decision_boundaries(X_train_2d, y_train, svm_2d, \"SVM Decision Boundary (2D)\")\n",
    "plot_decision_boundaries(X_train_2d, y_train, mlp_2d, \"MLP Decision Boundary (2D)\")\n",
    "# Plot the metric\n",
    "def plot_metrics(metrics, title):\n",
    "    labels = list(metrics.keys())\n",
    "    precision = [metrics[label]['Precision'] for label in labels]\n",
    "    recall = [metrics[label]['Recall'] for label in labels]\n",
    "    f1_score = [metrics[label]['F1-Score'] for label in labels]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(x - width, precision, width, label='Precision')\n",
    "    ax.bar(x, recall, width, label='Recall')\n",
    "    ax.bar(x + width, f1_score, width, label='F1-Score')\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
=======
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9EPf5TYc1t4",
        "outputId": "89f4512e-7fb9-4f66-8edd-8dde1595c693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== MLP Classifier ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         afd       0.54      0.52      0.53       411\n",
            "         cdu       0.46      0.64      0.53      1138\n",
            "         csu       0.56      0.58      0.57      1355\n",
            "         fdp       0.87      0.79      0.83       841\n",
            "      gruene       0.71      0.56      0.63       532\n",
            "       linke       0.49      0.43      0.46       845\n",
            "         spd       0.49      0.31      0.38       563\n",
            "\n",
            "    accuracy                           0.57      5685\n",
            "   macro avg       0.59      0.55      0.56      5685\n",
            "weighted avg       0.58      0.57      0.57      5685\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. MLP Classifier\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=1000,\n",
        "    alpha=0.0001,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "report_mlp = classification_report(y_test, y_pred_mlp)\n",
        "metrics_mlp = extract_metrics(report_mlp)\n",
        "print(\"=== MLP Classifier ===\")\n",
        "print(report_mlp)"
      ]
>>>>>>> 15865582465bc208a31ae6df46c4c4be5ca9ebe6
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL7AP-5ig2Qc",
        "outputId": "4b9b23e0-1931-4729-acfb-3d862e45c594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Modellvergleich ===\n",
            "         Random Forest   SVM   MLP  XGBoost  Average\n",
            "afd               0.56  0.48  0.53     0.57     0.54\n",
            "cdu               0.53  0.52  0.53     0.55     0.53\n",
            "csu               0.60  0.54  0.57     0.62     0.58\n",
            "fdp               0.83  0.80  0.83     0.85     0.83\n",
            "gruene            0.64  0.62  0.63     0.68     0.64\n",
            "linke             0.50  0.36  0.46     0.52     0.46\n",
            "spd               0.46  0.41  0.38     0.48     0.43\n",
            "Average           0.59  0.53  0.56     0.61     0.57\n"
          ]
        }
      ],
      "source": [
        "metrics_all = {}\n",
        "\n",
        "for label in metrics_rf:\n",
        "    metrics_all[label] = {\n",
        "        'Random Forest': metrics_rf[label]['F1-Score'],\n",
        "        'SVM': metrics_svm[label]['F1-Score'],\n",
        "        'MLP': metrics_mlp[label]['F1-Score'],\n",
        "        'XGBoost': metrics_xgb[label]['F1-Score']\n",
        "    }\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_all).T\n",
        "\n",
        "metrics_df[\"Average\"] = metrics_df.mean(axis=1)\n",
        "\n",
        "average_row = metrics_df.mean(numeric_only=True)\n",
        "average_row.name = \"Average\"\n",
        "metrics_df = pd.concat([metrics_df, average_row.to_frame().T])\n",
        "\n",
        "#Output\n",
        "print(\"=== Modellvergleich ===\")\n",
        "print(metrics_df.round(2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}